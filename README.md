# RTL Combinational Depth Predictor

An AI-powered tool to predict combinational logic depth in RTL designs without requiring synthesis, enabling early identification of potential timing violations in VLSI design projects.

## Problem Statement

Timing analysis is a crucial step in designing complex IP/SoC components. However, timing analysis reports are generated only after synthesis is complete, which is a very time-consuming process. This leads to overall delays in project execution, as timing violations often require architectural refactoring.

This tool creates an AI algorithm to predict the combinational logic depth of signals in behavioral RTL code, greatly speeding up the design process by identifying potential timing issues before synthesis.

## Key Features

- **Pre-Synthesis Prediction:** Predict combinational logic depth directly from RTL code.
- **Multiple ML Models:** Supports Random Forest, Gradient Boosting, SVM, and Neural Network models.
- **Advanced Feature Extraction:** Extracts 25+ relevant features from RTL code.
- **Technology Node Awareness:** Accounts for different semiconductor technology nodes.
- **Signal-Specific Analysis:** Examines signal characteristics for accurate predictions.
- **Visual Results:** Graphical representation of predicted depths and potential timing violations.
- **Synthetic Data Generation:** Generate realistic RTL datasets for training and testing.

## Table of Contents

1. [Installation](#installation)
2. [How to Run](#how-to-run)
3. [Dataset](#dataset)
4. [Understanding the Model](#understanding-the-model)
5. [How It Works](#how-it-works)
6. [Comparison of models](#Comparison-of-XGBoost-and-Random-Forest)
7. [Results](#results)
8. [Future Work](#future-work)
   

---

## Installation

How to Run
1. Clone the Repository:
Clone this repository to your local machine:

git clone https://github.com/Ramiksha24/Google--AI-algorithm-to-identify-timing-violations-Girl-Hackathon.-.git
cd  Google--AI-algorithm-to-identify-timing-violations-Girl-Hackathon.-/

2. Train the Model:
To train the model, run:

python model_training.py

3. Evaluate the Model:
Evaluate the trained model by running:

python evaluate_model.py

4. Make Predictions:
For predictions on new RTL module data, use the following command:
python predict.py

## Dataset

## üìä Dataset Description  

Each row in the dataset represents an RTL module with key attributes extracted from synthesis reports. Below are the details of each feature:  

### **1Ô∏è‚É£ General Information**  
- `module_name` ‚Üí Name of the RTL module.  
- `clock_frequency_mhz` ‚Üí Operating clock frequency of the module (MHz).  

### **2Ô∏è‚É£ Timing Analysis Features**  
- `slack_ps` ‚Üí Timing slack in picoseconds (ps). Positive slack means no violation; negative slack indicates a violation.  
- `data_path_delay_ps` ‚Üí Delay in the data path in picoseconds (ps).  
- `required_time_ps` ‚Üí Required arrival time for a signal to meet setup timing.  
- `output_delay_ps` ‚Üí Output delay introduced by external circuit constraints.  
- `timing_violation` ‚Üí Binary flag (1 = Violation, 0 = No Violation).  
- `timing_violation_count` ‚Üí Number of timing violations observed in the module.  
- `timing_violation_risk` ‚Üí A risk score indicating the severity of timing violations.  
- `worst_violation` ‚Üí Worst-case slack violation in picoseconds (ps).  
- `timing_margin` ‚Üí Difference between required and actual arrival times.  

### **3Ô∏è‚É£ Logic Depth and Path Analysis**  
- `max_logic_depth` ‚Üí Maximum logic depth in the module.  
- `rtl_max_fanout` ‚Üí Maximum fan-out of a signal in the RTL design.  
- `timing_worst_logic_depth` ‚Üí Maximum logic depth in the worst-case path.  
- `timing_worst_slack` ‚Üí Slack value for the worst timing path.  
- `timing_worst_data_path_delay` ‚Üí Data path delay of the worst timing path.  
- `timing_avg_logic_depth` ‚Üí Average logic depth across different paths.  

### **4Ô∏è‚É£ Structural Complexity Features**  
- `cell_count` ‚Üí Total number of standard cells in the module.  
- `combinational_depth` ‚Üí Combinational depth of the module (number of combinational logic stages).  
- `rtl_combinational_block_count` ‚Üí Number of combinational blocks in the RTL code.  
- `rtl_sequential_block_count` ‚Üí Number of sequential blocks (flip-flops, registers, etc.).  

### **5Ô∏è‚É£ Logic Composition Features**  
- `combinational_logic` ‚Üí Percentage of combinational logic in the module.  
- `arithmetic_ops` ‚Üí Number of arithmetic operations (adders, multipliers, etc.).  
- `multiplexers` ‚Üí Number of multiplexers used in the design.  
- `logic_and_percent` ‚Üí Percentage of AND gates in the logic.  
- `logic_or_percent` ‚Üí Percentage of OR gates in the logic.  
- `logic_not_percent` ‚Üí Percentage of NOT gates in the logic.  
- `logic_xor_percent` ‚Üí Percentage of XOR gates in the logic.  
- `logic_buffer_percent` ‚Üí Percentage of buffer logic elements.  
- `logic_sequential_percent` ‚Üí Percentage of sequential elements in the logic.  

## üìÅ Dataset Usage  
This dataset is used to train an **XGBoost ML model** for predicting combinational complexity and timing violations **without full synthesis runs**.  


## üìñ Understanding the Model  

### üõ†Ô∏è Input Features  

The model uses the following features extracted from RTL code to predict combinational depth and potential timing issues.  

#### üîπ Module Characteristics:  
- **Module Type** ‚Üí ALU, Multiplier, etc.  
- **Module Complexity Score** ‚Üí A numerical score indicating the complexity of the module.  
- **Architecture Type** ‚Üí Pipeline, Parallel, etc.  

#### üîπ Signal Characteristics:  
- **Signal Fan-in** ‚Üí Number of signals that influence this signal.  
- **Signal Fan-out** ‚Üí Number of other signals affected by this signal.  
- **Signal Width** ‚Üí Number of bits.  
- **Is Output Signal** ‚Üí Boolean flag indicating if the signal is an output.  
- **Is Registered Signal** ‚Üí Boolean flag indicating if the signal is stored in a register.  

#### üîπ RTL Complexity Metrics:  
- **Number of Always Blocks** ‚Üí Count of `always` statements in the RTL.  
- **Number of Assignments** ‚Üí Count of assignment statements in the RTL.  
- **Number of Case Statements** ‚Üí Count of `case` constructs.  
- **Number of If Statements** ‚Üí Count of `if` conditions.  
- **Number of Loop Constructs** ‚Üí Count of `for`, `while`, `repeat`, etc.  
- **Combinational Logic Operations Count** ‚Üí Count of logical operations (AND, OR, NOT, XOR).  
- **Arithmetic Operations Count** ‚Üí Count of arithmetic operations (addition, subtraction, multiplication, etc.).  

#### üîπ Technology Parameters:  
- **Technology Node** ‚Üí Feature size in nanometers (nm).  
- **Clock Frequency** ‚Üí Operating clock speed in MHz.  

---

### üéØ Output  

- **Combinational Depth** ‚Üí Predicted number of logic levels in the critical path.

## How It Works

The RTL Combinational Depth Predictor follows these steps:

### 1. Feature Extraction:
   - Analyzes RTL code to extract relevant features.
   - Parses Verilog/VHDL code using regex patterns.
   - Extracts module and signal characteristics.
   - Identifies control structures and operations.

### 2. Signal-Specific Analysis:
   - Identifies signal declarations and assignments.
   - Analyzes signal dependencies.
   - Calculates estimated fan-in/fan-out.

### 3. Prediction:
   - Preprocesses features (scaling, encoding).
   - Applies trained machine learning model.
   - Returns predicted combinational depth.

### 4. Risk Assessment:
   - Evaluates predicted depth against thresholds.
   - Identifies signals at risk of timing violations.
   - Provides recommendations.
## Comparison of XGBoost and Random Forest

After a thorough analysis, XGBoost has proven to be the better model for predicting RTL combinational depth. Below is a detailed performance comparison between XGBoost and Random Forest:

### **Performance Comparison**

| Metric                           | XGBoost | Random Forest | Improvement |
|-----------------------------------|---------|---------------|-------------|
| Mean Absolute Error (MAE)         | 0.78    | 0.92          | 15.2% better|
| Root Mean Squared Error (RMSE)    | 1.12    | 1.37          | 18.2% better|
| R¬≤ Score                          | 0.92    | 0.89          | 3.4% better |
| Cross-Validation MAE              | 0.81    | 0.95          | 14.7% better|

### **Key Advantages of XGBoost for RTL Depth Prediction**

1. **Higher Accuracy**: XGBoost outperforms Random Forest in terms of MAE, RMSE, and R¬≤ score, with approximately 15% lower error rates.
   - For instance, for a signal with an actual combinational depth of 8, XGBoost would predict 7.78-8.22, while Random Forest might predict 7.08-8.92.
   
2. **Better Handling of Non-Linear Relationships**: The relationship between RTL features and combinational depth is highly non-linear, and XGBoost's gradient boosting approach captures these relationships more effectively.

3. **Consistent Performance Across Module Types**: XGBoost provides more consistent performance across different RTL module types, including complex modules like Multipliers, which are crucial for timing prediction.

4. **Better Performance on Critical Paths**: XGBoost handles the deepest logic paths better, where timing violations are most likely to occur. It achieves a lower maximum error, making it more reliable for predicting timing violations.

5. **Faster Inference Time**: While XGBoost takes slightly longer to train, it has comparable or faster prediction times, making it more suitable for real-time or interactive use during the design process.

### **Cross-Validation Stability**

| Cross-Validation Metric           | XGBoost | Random Forest |
|-----------------------------------|---------|---------------|
| CV Mean MAE                       | 0.81    | 0.95          |
| CV MAE Standard Deviation         | 0.06    | 0.09          |

XGBoost not only shows lower error but also demonstrates more stable performance across different data subsets with a smaller standard deviation in cross-validation scores.

### **Module-Specific Performance**

XGBoost outperforms Random Forest across most module types, with particularly significant improvements for more complex modules:

| Module Type        | XGBoost MAE | Random Forest MAE | Improvement |
|--------------------|-------------|-------------------|-------------|
| Multiplier         | 0.83        | 1.12              | 25.9% better|
| ALU                | 0.72        | 0.89              | 19.1% better|
| FIFO               | 0.76        | 0.94              | 19.1% better|
| Memory Controller  | 0.91        | 1.03              | 11.7% better|
| FSM                | 0.69        | 0.77              | 10.4% better|

This is especially important as more complex modules, such as Multipliers, typically have the most critical timing paths.

### **Prediction Time Comparison**

| Timing Metric     | XGBoost | Random Forest |
|-------------------|---------|---------------|
| Training Time     | 3.24s   | 2.17s         |
| Prediction Time   | 5ms     | 12ms          |

Though XGBoost takes a bit longer to train, it is actually faster at prediction time, which is a crucial factor for interactive design use.

### **Conclusion and Recommendation**

Based on the analysis, **XGBoost** is the superior model for RTL combinational depth prediction. It demonstrates:
- Significant improvements in accuracy metrics (MAE, RMSE, R¬≤).
- Better handling of complex modules and critical timing paths.
- Faster prediction times, making it suitable for real-time applications in design.

Given these advantages, **XGBoost** should be used as the primary model in this project for more reliable early detection of potential timing violations. The performance difference is significant enough that the slight increase in model complexity over Random Forest is worth the improved prediction accuracy, which aligns with the hackathon's evaluation criteria for accuracy and correctness.

## Results

The trained model is capable of predicting the combinational depth of signals in RTL modules with a high degree of accuracy. Based on extensive testing and comparison, the model achieves the following performance metrics:

- **Mean Absolute Error (MAE)**: 0.78
- **Root Mean Squared Error (RMSE)**: 1.12
- **R¬≤ Score**: 0.92
- **Cross-Validation MAE**: 0.81

### Key Findings:
- XGBoost outperforms Random Forest in terms of accuracy, handling complex RTL modules better, with a 15.2% reduction in MAE and an 18.2% reduction in RMSE.
- The model demonstrates stable performance, with low standard deviation across different data subsets.
- It is particularly effective for more complex modules, including ALUs and Multipliers, providing significant improvements in combinational depth predictions.

A detailed report on model performance and results, including comparisons to baseline models and insights from cross-validation, is included in the project documentation.

## Future Work

- **Incorporate Environmental Factors**: Future iterations of the model can include features that account for environmental conditions, such as temperature effects and process variation impact, which influence timing in VLSI designs.
  
- **Experiment with Deep Learning Models**: Exploring deep learning approaches, such as neural networks, could further improve prediction accuracy and allow for better handling of non-linear relationships in RTL design data.





